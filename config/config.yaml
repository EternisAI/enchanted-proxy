model_router:
  providers:
  # Self-hosted models. Base URL is defined in per-model provider specs.
  - name: Eternis
    api_key_env_var: ETERNIS_INFERENCE_API_KEY

  - name: NEAR AI
    api_key_env_var: NEAR_API_KEY
    base_url: https://cloud-api.near.ai/v1

  - name: Tinfoil
    api_key_env_var: TINFOIL_API_KEY
    base_url: https://inference.tinfoil.sh/v1

  - name: OpenAI
    api_key_env_var: OPENAI_API_KEY
    base_url: https://api.openai.com/v1

  # API key is resolved at route time based on platform (mobile/desktop, defaults to mobile).
  # Default provider for unknown models.
  - name: OpenRouter
    base_url: https://openrouter.ai/api/v1

  models:
  # DeepSeek R1 - Free & Pro - via Tinfoil (1× multiplier)
  - name: deepseek-ai/DeepSeek-R1-0528
    aliases:
    - deepseek/deepseek-r1-0528
    - deepseek-r1-0528
    - deepseek-r1
    providers:
    - name: Tinfoil
      model: deepseek-r1-0528

  # Llama 3.3 70B - Free & Pro - via Tinfoil (1× multiplier)
  - name: meta-llama/Llama-3.3-70B
    aliases:
    - llama-3.3-70b
    - llama3-3-70b
    providers:
    - name: Tinfoil
      model: llama3-3-70b

  # GLM-4.6 - Free & Pro - via local/our IPs, fallback to NEAR AI (0.6× multiplier)
  - name: zai-org/GLM-4.6
    aliases:
    - z-ai/glm-4.6
    - glm-4.6
    token_multiplier: 0.6
    providers:
    - name: Eternis
      base_url: http://127.0.0.1:20001/v1
      fallback:
        trigger:
          # Trigger if any of the following conditions held true throughout the last 2-minute window:
          # TTFT p95 > 1s, ITL p95 > 150ms, queue time p95 > 750ms, queue non-empty at least 75% of the time, KV cache > 85%
          # Keep the traffic on fallback for at least 2m
          dwell_time: 2m
          query: >-
            (min_over_time((((histogram_quantile(0.95, sum by (le) (rate(vllm:time_to_first_token_seconds_bucket{model_name="zai-org/GLM-4.6"}[1m]))) > bool 1) and on() (sum(rate(vllm:time_to_first_token_seconds_bucket{model_name="zai-org/GLM-4.6", le="+Inf"}[1m])) > bool 0)) or on() vector(0))[2m:15s]) == 1)
            or (min_over_time((((histogram_quantile(0.95, sum by (le) (rate(vllm:inter_token_latency_seconds_bucket{model_name="zai-org/GLM-4.6"}[1m]))) > bool 0.15) and on() (sum(rate(vllm:inter_token_latency_seconds_bucket{model_name="zai-org/GLM-4.6", le="+Inf"}[1m])) > bool 0)) or on() vector(0))[2m:15s]) == 1)
            or (min_over_time((((histogram_quantile(0.95, sum by (le) (rate(vllm:request_queue_time_seconds_bucket{model_name="zai-org/GLM-4.6"}[1m]))) > bool 0.75) and on() (sum(rate(vllm:request_queue_time_seconds_bucket{model_name="zai-org/GLM-4.6", le="+Inf"}[1m])) > bool 0)) or on() vector(0))[2m:15s]) == 1)
            or (avg_over_time((sum(vllm:num_requests_waiting{model_name="zai-org/GLM-4.6"}) >= bool 1)[2m:15s]) > 0.75)
            or (min_over_time((max(vllm:kv_cache_usage_perc{model_name="zai-org/GLM-4.6"}))[2m:15s]) > 0.85)
        # Recover if all of the following conditions held true throughout the last 5-minute window:
        # TTFT p95 < 500ms, ITL p95 < 75ms, queue time p95 < 350ms, Queue non-empty at most 40% of the time, KV cache < 80%
        # Treat missing data points as satisfying the conditions (when traffic is shifted away)
        # Keep the traffic from fallback for at least 1m
        recover:
          dwell_time: 1m
          query: >-
            (min_over_time(((((sum(rate(vllm:time_to_first_token_seconds_bucket{model_name="zai-org/GLM-4.6", le="+Inf"}[1m])) == bool 0) + (histogram_quantile(0.95, sum by (le) (rate(vllm:time_to_first_token_seconds_bucket{model_name="zai-org/GLM-4.6"}[1m]))) < bool 0.5)) >= bool 1) or on() vector(1))[5m:15s]) == 1)
            and (min_over_time(((((sum(rate(vllm:inter_token_latency_seconds_bucket{model_name="zai-org/GLM-4.6", le="+Inf"}[1m])) == bool 0) + (histogram_quantile(0.95, sum by (le) (rate(vllm:inter_token_latency_seconds_bucket{model_name="zai-org/GLM-4.6"}[1m]))) < bool 0.075)) >= bool 1) or on() vector(1))[5m:15s]) == 1)
            and (min_over_time(((((sum(rate(vllm:request_queue_time_seconds_bucket{model_name="zai-org/GLM-4.6", le="+Inf"}[1m])) == bool 0) + (histogram_quantile(0.95, sum by (le) (rate(vllm:request_queue_time_seconds_bucket{model_name="zai-org/GLM-4.6"}[1m]))) < bool 0.35)) >= bool 1) or on() vector(1))[5m:15s]) == 1)
            and (avg_over_time((sum(vllm:num_requests_waiting{model_name="zai-org/GLM-4.6"}) >= bool 1)[5m:15s]) < 0.4)
            and (max_over_time((max(vllm:kv_cache_usage_perc{model_name="zai-org/GLM-4.6"}))[5m:15s]) < 0.8)
    - name: NEAR AI

  # Qwen3 30B A3B - Free & Pro - via NEAR AI (0.8× multiplier)
  - name: Qwen/Qwen3-30B-A3B-Instruct-2507
    aliases:
    - qwen3-30b
    - qwen-30b
    token_multiplier: 0.8
    providers:
    - name: NEAR AI

  # Dolphin Mistral (Venice) - Free & Pro - via GCP self-hosted (0.5× multiplier)
  - name: dphn/Dolphin-Mistral-24B-Venice-Edition
    aliases:
    - dolphin-mistral-eternis
    - dolphin-mistral
    token_multiplier: 0.5
    providers:
    - name: Eternis
      base_url: http://34.30.193.13:8000/v1

  # GLM-4.7 - Dev/Testing - via OpenRouter (1× multiplier)
  - name: z-ai/glm-4.7
    aliases:
    - zai-org/GLM-4.7
    - zhipu/glm-4.7
    - glm-4.7
    token_multiplier: 1.0
    providers:
    - name: OpenRouter

  # GPT-4.1 - Pro only - via OpenRouter (4× multiplier)
  - name: openai/gpt-4.1
    aliases:
    - gpt-4.1
    token_multiplier: 4.0
    providers:
    - name: OpenRouter

  # GPT-5 - Pro only - via OpenRouter (6× multiplier)
  - name: openai/gpt-5
    aliases:
    - gpt-5
    token_multiplier: 6.0
    providers:
    - name: OpenRouter

  # GPT-5 Pro - Pro only - via OpenAI directly using Responses API (50× multiplier)
  - name: openai/gpt-5-pro
    aliases:
    - gpt-5-pro
    token_multiplier: 50.0
    providers:
    - name: OpenAI
      model: gpt-5-pro
      api_type: responses

  # Legacy OpenAI models - maintain backward compatibility with existing multipliers
  - name: openai/gpt-4
    aliases:
    - gpt-4
    providers:
    - name: OpenAI
      model: gpt-4

  - name: openai/gpt-4-turbo
    aliases:
    - gpt-4-turbo
    providers:
    - name: OpenAI
      model: gpt-4-turbo

  - name: openai/gpt-3.5-turbo
    aliases:
    - gpt-3.5-turbo
    providers:
    - name: OpenAI
      model: gpt-3.5-turbo

  - name: openai/o1-preview
    aliases:
    - o1-preview
    providers:
    - name: OpenAI
      model: o1-preview

  - name: openai/o1-mini
    aliases:
    - o1-mini
    providers:
    - name: OpenAI
      model: o1-mini

  - name: openai/o3-mini
    aliases:
    - o3-mini
    providers:
    - name: OpenAI
      model: o3-mini

  # Fallback: OpenRouter handles unknown models (including Claude via OpenRouter)
  - name: '*'
    providers:
    - name: OpenRouter
